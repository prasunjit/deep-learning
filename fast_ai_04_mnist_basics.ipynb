{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8V7BqOMBYVsH"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *\n",
        "matplotlib.rc('image', cmap='Greys')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.MNIST_SAMPLE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "JtdI8yVeSKxb",
        "outputId": "c220352d-2047-42cc-9d99-d1a831800f88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='3219456' class='' max='3214948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.14% [3219456/3214948 00:00&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Path.BASE_PATH = path"
      ],
      "metadata": {
        "id": "u2tZcUnJSPnZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path.ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "webFAdj0SUEC",
        "outputId": "d87e92be-d52f-4c4e-84b5-d216789fb2d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#3) [Path('valid'),Path('labels.csv'),Path('train')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(path/'train').ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaVzbc1tSg-k",
        "outputId": "e40c42f4-2fab-497f-91b6-644a4ae8472d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [Path('train/7'),Path('train/3')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threes = (path/'train'/'3').ls().sorted()\n",
        "sevens = (path/'train'/'7').ls().sorted()\n",
        "threes\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "901oBAtpSzN3",
        "outputId": "d6acb961-7ef4-4ca1-c58e-02abe2523496"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.png'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.png'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.png'),Path('train/3/10091.png'),Path('train/3/10093.png'),Path('train/3/10097.png'),Path('train/3/10099.png'),Path('train/3/10116.png'),Path('train/3/10125.png'),Path('train/3/10137.png'),Path('train/3/10141.png'),Path('train/3/10144.png'),Path('train/3/10155.png'),Path('train/3/10161.png')...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "im3_path = threes[1]\n",
        "im3 = Image.open(im3_path)\n",
        "im3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "l5Msqt40S1qH",
        "outputId": "8590458e-d0db-4e42-ea65-3b7d895c9bb2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA9UlEQVR4AWNgGGSAEe4e2Upda8b/mwSvnloAF4MyDJY9+Pv31ZG/QPASTS72y8+/u/W4OFi4DiFLsoCV8XEyvCy9BGT++cfAsBlNJ7OICD9YSPvB369eaJJw7s+/X+vgHBQGX+r9vz9qUIRgHO5l74FO3S0H46PQAj9+gTzy4mo+E4o4lKOdlHQBJL9bDJssAwOncTlQ1ge7JAMD4/a/f7uhkhjm///PwHAXl87Qn3//quCQtLn29+9abuySSd///n3EiVVOa9ofYKSYostpFWvZFM//8Pfvr/WS6HIMu0GeB4KjYRhSDAzpYKkXzohUg0URtYQA/HZrR+ekLi0AAAAASUVORK5CYII=\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APCdP02+1a8W006znu7l+VigjLsQOpwO1dlb/CjVnkS3vNb8O6dqDkKtheaiFn3Hou1QcE8YBPcVg+LfCGqeCtYXS9WEPnvEJlML7lKEkA547qawatafqd/pN2LvTb24s7lQQJbeQowB6jI5r1D4Y6b4OvdXtdf8S+K45NY85phY3W6MeaCdrSTNw3IDcd8ZzyKx/i5pXiiLxMNZ8RC1kjvx/os1nJvh2KOFU8HgEHkc5zXntWdP0681a/hsbC3kubqY7Y4oxlmPXiu68OfBzxPq1yJNVtW0XTI/mnu73CbF74UkEn64HvVn4r+LdI1GDR/C3h2QzaTosXli5JJ858BeM9QAOvck9sV5nU1rdXFjdR3VpPLb3ETbo5YnKOh9QRyDV7UfE2v6vbi31PXNSvYA24RXN3JIufXDEjNZdFf/2Q==\n"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "im3_t = tensor(im3)\n",
        "df = pd.DataFrame(im3_t[4:18,4:20])\n",
        "df\n",
        "# df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n",
        "tensor(im3).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0qwZg_8TAg-",
        "outputId": "11724554-9beb-4f43-f320-abe2225052c1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seven_lists = [tensor(Image.open(o)) for o in sevens]\n",
        "three_lists = [tensor(Image.open(o)) for o in threes]\n",
        "# len(three_tensors),len(seven_tensors)\n",
        "# seven_tensors.shape"
      ],
      "metadata": {
        "id": "rJIJ_2Q8mxUy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_sevens = torch.stack(seven_lists).float()/255\n",
        "stacked_threes = torch.stack(three_lists).float()/255\n",
        "stacked_threes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdns1pMinI-6",
        "outputId": "b8529a45-3c52-46f1-faf5-c1ca3d01b818"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6131, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFirst few elements of the first 'seven' image:\")\n",
        "print(stacked_sevens[0, :10, :10])  # First image, first 5 rows, first 5 columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYWp3RBAm9oD",
        "outputId": "8b6a6919-d69a-4502-a13b-e0bbacba3ae5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First few elements of the first 'seven' image:\n",
            "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0824, 0.2000, 0.8353, 0.9961, 0.9882,\n",
            "         0.9882],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0824, 0.6314, 0.9804, 0.9804, 0.9882, 0.9804,\n",
            "         0.9804],\n",
            "        [0.0000, 0.0000, 0.0000, 0.2000, 0.9804, 0.9804, 0.9804, 0.9882, 0.7412,\n",
            "         0.7451]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean3 = stacked_threes.mean(0)\n",
        "show_image(mean3);"
      ],
      "metadata": {
        "id": "prHvytYVIBuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mean7 = stacked_sevens.mean(0)\n",
        "show_image(mean7);"
      ],
      "metadata": {
        "id": "gBJGwPP6IF87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you have three images of size 2x2:\n",
        "For simplicity we took just 4 pixel values and they are not normalized\n",
        "\n",
        "```\n",
        "Image 1:\n",
        "\n",
        "[[10, 20],\n",
        " [30, 40]]\n",
        "Image 2:\n",
        "\n",
        "[[15, 25],\n",
        " [35, 45]]\n",
        "Image 3:\n",
        "\n",
        "[[20, 30],\n",
        " [40, 50]]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "When you stack these images into a rank-3 tensor stacked_images with shape (3, 2, 2), it looks conceptually like this:\n",
        "```\n",
        "[\n",
        "  [[10, 20],  # Image 1\n",
        "   [30, 40]],\n",
        "\n",
        "  [[15, 25],  # Image 2\n",
        "   [35, 45]],\n",
        "\n",
        "  [[20, 30],  # Image 3\n",
        "   [40, 50]]\n",
        "]\n",
        "```\n",
        "Now, if you calculate average_image = stacked_images.float().mean(dim=0), here's what happens for each pixel location:\n",
        "\n",
        "\n",
        "\n",
        "> Pixel at (0, 0) (top-left): The values across the images are 10, 15, and 20. The average is (10 + 15 + 20) / 3 = 15.  \n",
        "Pixel at (0, 1) (top-right): The values are 20, 25, and 30. The average is (20 + 25 + 30) / 3 = 25.  \n",
        "Pixel at (1, 0) (bottom-left): The values are 30, 35, and 40. The average is (30 + 35 + 40) / 3 = 35.  \n",
        "Pixel at (1, 1) (bottom-right): The values are 40, 45, and 50. The average is (40 + 45 + 50) / 3 = 45.\n",
        "\n",
        "\n",
        "\n",
        "So, the resulting average_image tensor would be:\n",
        "```\n",
        "[[15., 25.],\n",
        " [35., 45.]]\n",
        "```\n",
        "In summary:\n",
        "\n",
        "stacked_images.mean(dim=0) calculates the mean along the first dimension (dimension 0), which represents the different images in the stack. For each fixed position in the other dimensions (height and width), it takes the average of the values across all the images at that specific location.\n",
        "\n",
        "Therefore, the resulting average_image is a single image where each pixel's intensity is the average intensity of that same pixel location across all the input images. It's a way to see the \"prototypical\" intensity pattern for that class of images."
      ],
      "metadata": {
        "id": "udp6G24XIw8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist_distance(a,b): return (a-b).abs().mean((-1,-2))"
      ],
      "metadata": {
        "id": "As4nrYGQMVtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_3_dist = mnist_distance(valid_3_tens, mean3)\n",
        "valid_3_dist, valid_3_dist.shape"
      ],
      "metadata": {
        "id": "MfZZB__zIIx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of above code\n",
        "\n",
        "Matrix Example for the First Image\n",
        "To illustrate what's happening with the .mean((-1, -2)) operation, let's consider a simplified example focusing on the first image in valid_3_tens.\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "valid_3_tens has a shape of (1010, 28, 28).  \n",
        "\n",
        "We're looking at valid_3_tens[0], which is the first image in the tensor.  This has a shape of (28, 28).\n",
        "\n",
        "mean3 also has a shape of (28, 28).\n",
        "\n",
        "\n",
        "\n",
        "1.   Subtraction and Absolute Value.   \n",
        "When we calculate (valid_3_tens - mean3).abs(), we perform element-wise subtraction between the pixel values of the first image in valid_3_tens and the corresponding pixel values in mean3.  The .abs() ensures we're dealing with the magnitude of the differences.  \n",
        "Let's imagine a simplified 3x3 example (instead of 28x28) to visualize this.  Suppose the top-left portion of valid_3_tens[0] and mean3 look like this:\n",
        "\n",
        "\n",
        "\n",
        "      valid_3_tens[0] (3x3 example):\n",
        "      ```\n",
        "          [[100, 110, 120],\n",
        "          [130, 140, 150],\n",
        "          [160, 170, 180]]\n",
        "\n",
        "          mean3 (3x3 example):\n",
        "          [[90, 100, 110],\n",
        "          [120, 130, 140],\n",
        "          [150, 160, 170]]\n",
        "\n",
        "          Then, the result of (valid_3_tens[0] - mean3).abs() would be:\n",
        "\n",
        "          Absolute Differences (3x3 example):\n",
        "          [[10, 10, 10],\n",
        "          [10, 10, 10],\n",
        "          [10, 10, 10]]\n",
        "      ```\n",
        "\n",
        "\n",
        "2.   Applying .mean((-1, -2))\n",
        "\n",
        "    Now, we apply .mean((-1, -2)) to this absolute difference matrix.\n",
        "\n",
        "    -1 refers to the last dimension (width, size 3 in our example).\n",
        "\n",
        "    -2 refers to the second-to-last dimension (height, size 3 in our example).\n",
        "\n",
        "    This means we calculate the average of all the elements in the 3x3 matrix:\n",
        "\n",
        "    Average = (10 + 10 + 10 + 10 + 10 + 10 + 10 + 10 + 10) / 9 = 10\n",
        "\n",
        "    In the actual 28x28 case, .mean((-1, -2)) would calculate the average of all 784 (28 * 28) absolute differences.\n",
        "\n",
        "3. Result for the First Image\n",
        "\n",
        "    The result of this operation is a single number (a scalar).  In our 3x3 example, the result is 10.  In the real 28x28 scenario, it would be a single number representing the average absolute pixel difference between the first image in valid_3_tens and the mean3 image.  This value would then be stored in valid_3_dist[0]."
      ],
      "metadata": {
        "id": "Ys66xo_LMCgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "accuracy_3s =      is_3(valid_3_tens).float() .mean()\n",
        "accuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n",
        "\n",
        "accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Why Combine the Accuracies?\n",
        "\n",
        "The combined accuracy provides a summary measure of how well your classifier performs on a subset of your data (in this case, the digits \"3\" and \"7\").  It's useful for:\n",
        "\n",
        "Overall Performance: Getting a general sense of how well the model distinguishes between these two classes.\n",
        "\n",
        "Comparison: Comparing the performance of different classifiers or different versions of the same classifier.\n",
        "\n",
        "Monitoring Training: Tracking how the accuracy on this subset changes during the training process."
      ],
      "metadata": {
        "id": "HT9sjXbBP1xh"
      }
    }
  ]
}